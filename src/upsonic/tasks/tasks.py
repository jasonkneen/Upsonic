from __future__ import annotations

import base64
import time
from pydantic import BaseModel
from typing import Any, List, Dict, Optional, Type, Union, Callable, Literal, TYPE_CHECKING
from upsonic.exceptions import FileNotFoundError
from upsonic.run.base import RunStatus

if TYPE_CHECKING:
    from upsonic.tools import ExternalToolCall
    from upsonic.agent.deepagent.tools.planning_toolkit import TodoList


CacheMethod = Literal["vector_search", "llm_call"]
CacheEntry = Dict[str, Any]

class Task(BaseModel):
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    description: str
    attachments: Optional[List[str]] = None
    tools: list[Any] = None
    response_format: Union[Type[BaseModel], type[str], None] = str
    response_lang: Optional[str] = "en"
    _response: Optional[Union[str, bytes]] = None
    context: Any = None
    _context_formatted: Optional[str] = None
    price_id_: Optional[str] = None
    task_id_: Optional[str] = None
    not_main_task: bool = False
    start_time: Optional[int] = None
    end_time: Optional[int] = None
    agent: Optional[Any] = None
    enable_thinking_tool: Optional[bool] = None
    enable_reasoning_tool: Optional[bool] = None
    _tool_calls: List[Dict[str, Any]] = None
    guardrail: Optional[Callable] = None
    guardrail_retries: Optional[int] = None
    is_paused: bool = False
    _tools_awaiting_external_execution: List[Any] = []
    status: Optional[RunStatus] = None
    enable_cache: bool = False
    cache_method: Literal["vector_search", "llm_call"] = "vector_search"
    cache_threshold: float = 0.7
    cache_embedding_provider: Optional[Any] = None
    cache_duration_minutes: int = 60
    _cache_manager: Optional[Any] = None
    _cache_hit: bool = False
    _original_input: Optional[str] = None
    _last_cache_entry: Optional[Dict[str, Any]] = None
    
    _run_id: Optional[str] = None

    _task_todos: Optional[Any] = None
    
    registered_task_tools: Dict[str, Any] = {}
    task_builtin_tools: List[Any] = []
        
    vector_search_top_k: Optional[int] = None
    vector_search_alpha: Optional[float] = None
    vector_search_fusion_method: Optional[Literal['rrf', 'weighted']] = None
    vector_search_similarity_threshold: Optional[float] = None
    vector_search_filter: Optional[Dict[str, Any]] = None

    @staticmethod
    def _is_file_path(item: Any) -> bool:
        """
        Check if an item is a valid file path.
        
        Args:
            item: Any object to check
            
        Returns:
            bool: True if the item is a string representing an existing file path
            
        Raises:
            FileNotFoundError: If the file path exists but cannot be accessed, or if it looks like a file path but doesn't exist
        """
        if not isinstance(item, str):
            return False
        
        import os
        
        # Check if it's a valid file path and the file exists
        try:
            if os.path.isfile(item):
                # Additional check to ensure file is readable
                if not os.access(item, os.R_OK):
                    raise FileNotFoundError(item, "File exists but is not readable")
                return True
            elif os.path.isdir(item):
                # It's a directory, not a file
                return False
            else:
                # Check if it looks like a file path but doesn't exist
                if (item.endswith(('.txt', '.pdf', '.docx', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv')) or 
                    ('/' in item or '\\' in item) and '.' in item):
                    raise FileNotFoundError(item, "File does not exist")
                return False
        except (TypeError, ValueError, OSError) as e:
            # If it's a string that looks like a file path but can't be accessed, raise error
            if isinstance(item, str) and (item.endswith(('.txt', '.pdf', '.docx', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv')) or '/' in item or '\\' in item):
                raise FileNotFoundError(item, f"Cannot access file: {str(e)}")
            return False
    
    @staticmethod
    def _is_folder_path(item: Any) -> bool:
        """
        Check if an item is a valid folder/directory path.
        
        Args:
            item: Any object to check
            
        Returns:
            bool: True if the item is a string representing an existing directory
            
        Raises:
            FileNotFoundError: If the folder path exists but cannot be accessed, or if it looks like a directory path but doesn't exist
        """
        if not isinstance(item, str):
            return False
        
        import os
        
        # Check if it's a valid directory path and the directory exists
        try:
            if os.path.isdir(item):
                # Additional check to ensure directory is readable
                if not os.access(item, os.R_OK):
                    raise FileNotFoundError(item, "Directory exists but is not readable")
                return True
            else:
                # Check if it looks like a directory path but doesn't exist
                # A path looks like a directory if it ends with / or \, or if it contains path separators
                if (item.endswith('/') or item.endswith('\\') or 
                    (('/' in item or '\\' in item) and not os.path.isfile(item))):
                    raise FileNotFoundError(item, "Directory does not exist")
                return False
        except (TypeError, ValueError, OSError) as e:
            # If it's a string that looks like a directory path but can't be accessed, raise error
            if isinstance(item, str) and (item.endswith('/') or item.endswith('\\') or '/' in item or '\\' in item):
                raise FileNotFoundError(item, f"Cannot access directory: {str(e)}")
            return False
    
    @staticmethod
    def _get_files_from_folder(folder_path: str) -> List[str]:
        """
        Recursively get all file paths from a folder.
        
        Args:
            folder_path: Path to the folder
            
        Returns:
            List[str]: List of all file paths in the folder and subfolders
            
        Raises:
            FileNotFoundError: If the folder cannot be accessed
        """
        import os
        
        files = []
        try:
            for root, dirs, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(root, filename)
                    files.append(file_path)
        except (OSError, PermissionError) as e:
            # If we can't access the folder, raise a proper error
            raise FileNotFoundError(folder_path, f"Cannot access folder: {str(e)}")
        
        return files
    
    @staticmethod
    def _extract_files_from_context(context: Any) -> tuple[Any, List[str]]:
        """
        Extract file paths from context and return cleaned context and file list.
        Also handles folders by extracting all files from them recursively.
        
        Args:
            context: The context parameter (can be a list, dict, or any other type)
            
        Returns:
            tuple: (cleaned_context, extracted_files)
                - cleaned_context: Context with file/folder paths removed
                - extracted_files: List of file paths found (including files from folders)
                
        Raises:
            FileNotFoundError: If any file or folder in context cannot be accessed
        """
        extracted_files = []
        
        # If context is None or empty, return as is
        if not context:
            return context, extracted_files
        
        # Handle list context
        if isinstance(context, list):
            cleaned_context = []
            for item in context:
                try:
                    if Task._is_file_path(item):
                        extracted_files.append(item)
                    elif Task._is_folder_path(item):
                        # Extract all files from the folder
                        folder_files = Task._get_files_from_folder(item)
                        extracted_files.extend(folder_files)
                    else:
                        cleaned_context.append(item)
                except FileNotFoundError:
                    # Re-raise the exception with context
                    raise
            return cleaned_context, extracted_files
        
        # Handle dict context - check values
        elif isinstance(context, dict):
            cleaned_context = {}
            for key, value in context.items():
                try:
                    if Task._is_file_path(value):
                        extracted_files.append(value)
                    elif Task._is_folder_path(value):
                        # Extract all files from the folder
                        folder_files = Task._get_files_from_folder(value)
                        extracted_files.extend(folder_files)
                    elif isinstance(value, list):
                        # Recursively process lists in dict values
                        cleaned_list = []
                        for item in value:
                            try:
                                if Task._is_file_path(item):
                                    extracted_files.append(item)
                                elif Task._is_folder_path(item):
                                    # Extract all files from the folder
                                    folder_files = Task._get_files_from_folder(item)
                                    extracted_files.extend(folder_files)
                                else:
                                    cleaned_list.append(item)
                            except FileNotFoundError:
                                # Re-raise the exception with context
                                raise
                        cleaned_context[key] = cleaned_list
                    else:
                        cleaned_context[key] = value
                except FileNotFoundError:
                    # Re-raise the exception with context
                    raise
            return cleaned_context, extracted_files
        
        # Handle single string that might be a file path or folder
        try:
            if Task._is_file_path(context):
                extracted_files.append(context)
                return [], extracted_files
            elif Task._is_folder_path(context):
                # Extract all files from the folder
                folder_files = Task._get_files_from_folder(context)
                extracted_files.extend(folder_files)
                return [], extracted_files
        except FileNotFoundError:
            raise
        
        else:
            return context, extracted_files

    def __init__(
        self, 
        description: str, 
        attachments: Optional[List[str]] = None,
        tools: list[Any] = None,
        response_format: Union[Type[BaseModel], type[str], None] = str,
        response: Optional[Union[str, bytes]] = None,
        context: Any = None,
        _context_formatted: Optional[str] = None,
        price_id_: Optional[str] = None,
        task_id_: Optional[str] = None,
        not_main_task: bool = False,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None,
        agent: Optional[Any] = None,
        response_lang: Optional[str] = None,
        enable_thinking_tool: Optional[bool] = None,
        enable_reasoning_tool: Optional[bool] = None,
        guardrail: Optional[Callable] = None,
        guardrail_retries: Optional[int] = None,
        is_paused: bool = False,
        _tools_awaiting_external_execution: List["ExternalToolCall"] = None,
        enable_cache: bool = False,
        cache_method: Literal["vector_search", "llm_call"] = "vector_search",
        cache_threshold: float = 0.7,
        cache_embedding_provider: Optional[Any] = None,
        cache_duration_minutes: int = 60,
        _task_todos: Optional[TodoList] = None,
        vector_search_top_k: Optional[int] = None,
        vector_search_alpha: Optional[float] = None,
        vector_search_fusion_method: Optional[Literal['rrf', 'weighted']] = None,
        vector_search_similarity_threshold: Optional[float] = None,
        vector_search_filter: Optional[Dict[str, Any]] = None,
    ):
        if guardrail is not None and not callable(guardrail):
            raise TypeError("The 'guardrail' parameter must be a callable function.")
        
        if cache_method not in ("vector_search", "llm_call"):
            raise ValueError("cache_method must be either 'vector_search' or 'llm_call'")
        
        if not (0.0 <= cache_threshold <= 1.0):
            raise ValueError("cache_threshold must be between 0.0 and 1.0")
        
        if enable_cache and cache_method == "vector_search" and cache_embedding_provider is None:
            try:
                from upsonic.embeddings.factory import auto_detect_best_embedding
                cache_embedding_provider = auto_detect_best_embedding()
            except Exception:
                raise ValueError("cache_embedding_provider is required when cache_method is 'vector_search'")
            
        if tools is None:
            tools = []
            
        if context is None:
            context = []

        if _tools_awaiting_external_execution is None:
            _tools_awaiting_external_execution = []
        
        try:
            context, extracted_files = self._extract_files_from_context(context)
        except FileNotFoundError as e:
            raise FileNotFoundError(e.file_path, f"File specified in context cannot be accessed: {e.reason}")
        
        if attachments is None:
            attachments = []
        
        if extracted_files:
            attachments.extend(extracted_files)
            
        super().__init__(**{
            "description": description,
            "attachments": attachments,
            "tools": tools,
            "response_format": response_format,
            "_response": response,
            "context": context,
            "_context_formatted": _context_formatted,
            "price_id_": price_id_,
            "task_id_": task_id_,
            "not_main_task": not_main_task,
            "start_time": start_time,
            "end_time": end_time,
            "agent": agent,
            "response_lang": response_lang,
            "enable_thinking_tool": enable_thinking_tool,
            "enable_reasoning_tool": enable_reasoning_tool,
            "guardrail": guardrail,
            "guardrail_retries": guardrail_retries,
            "_tool_calls": [],
            "is_paused": is_paused,
            "_tools_awaiting_external_execution": _tools_awaiting_external_execution,
            "enable_cache": enable_cache,
            "cache_method": cache_method,
            "cache_threshold": cache_threshold,
            "cache_embedding_provider": cache_embedding_provider,
            "cache_duration_minutes": cache_duration_minutes,
            "_cache_manager": None,  # Will be set by Agent
            "_cache_hit": False,
            "_original_input": description,
            "_last_cache_entry": None,
            "_task_todos": _task_todos or [],
            "registered_task_tools": {},  # Initialize empty tool registry
            "vector_search_top_k": vector_search_top_k,
            "vector_search_alpha": vector_search_alpha,
            "vector_search_fusion_method": vector_search_fusion_method,
            "vector_search_similarity_threshold": vector_search_similarity_threshold,
            "vector_search_filter": vector_search_filter,
        })
        
        self.validate_tools()

    @property
    def duration(self) -> Optional[float]:
        if self.start_time is None or self.end_time is None:
            return None
        return self.end_time - self.start_time

    @property
    def id(self) -> str:
        """Get the task ID. Auto-generates one if not set."""
        return self.task_id

    
    @property
    def is_problematic(self) -> bool:
        """
        Check if the task's run is problematic (paused, cancelled, or error).
        
        A problematic run requires continue_run_async() to be called instead of do_async().
        """
        if self.status is None:
            return False
        return self.status in (RunStatus.paused, RunStatus.cancelled, RunStatus.error)
    
    @property
    def is_completed(self) -> bool:
        """
        Check if the task's run is already completed.
        
        A completed task cannot be re-run or continued.
        """
        if self.status is None:
            return False
        return self.status == RunStatus.completed

    def validate_tools(self):
        """
        Validates each tool in the tools list.
        If a tool is a class and has a __control__ method, runs that method to verify it returns True.
        Raises an exception if the __control__ method returns False or raises an exception.
        """
        if not self.tools:
            return
            
        for tool in self.tools:
            # Check if the tool is a class
            if isinstance(tool, type) or hasattr(tool, '__class__'):
                # Check if the class has a __control__ method
                if hasattr(tool, '__control__') and callable(getattr(tool, '__control__')):

                        control_result = tool.__control__()

    def add_tools(self, tools: Union[Any, List[Any]]) -> None:
        """
        Add tools to the task's tool list.
        
        This method simply adds tools to self.tools without processing them.
        Tools are processed at runtime when the agent executes the task.
        
        Note: If plan_and_execute is added explicitly, it will be treated as a
        regular tool (not auto-managed by enable_thinking_tool).
        
        Args:
            tools: A single tool or list of tools to add
        """
        if not isinstance(tools, list):
            tools = [tools]
        
        # Initialize self.tools if it's None
        if self.tools is None:
            self.tools = []
        
        # Add tools to self.tools
        for tool in tools:
            if tool not in self.tools:
                self.tools.append(tool)
    
    def remove_tools(self, tools: Union[str, List[str], Any, List[Any]], agent: Any) -> None:
        """
        Remove tools from the task.
        
        This method requires an agent instance because task tools are registered
        at runtime (not in __init__), so we need access to the agent's ToolManager
        to properly remove tools from all relevant data structures.
        
        Supports removing:
        - Tool names (strings)
        - Function objects
        - Agent objects
        - MCP handlers (and all their tools)
        - Class instances (ToolKit or regular classes, and all their tools)
        - Builtin tools (AbstractBuiltinTool instances)
        
        Args:
            tools: Single tool or list of tools to remove (any type)
            agent: Agent instance for accessing ToolManager
        """
        if not isinstance(tools, list):
            tools = [tools]
        
        # Separate builtin tools from regular tools
        from upsonic.tools.builtin_tools import AbstractBuiltinTool
        builtin_tools_to_remove = []
        regular_tools_to_remove = []
        
        for tool in tools:
            if tool is not None and isinstance(tool, AbstractBuiltinTool):
                builtin_tools_to_remove.append(tool)
            else:
                regular_tools_to_remove.append(tool)
        
        # Handle regular tools through ToolManager
        removed_tool_names = []
        removed_objects = []
        
        if regular_tools_to_remove:
            # Call ToolManager to handle all removal logic for regular tools
            # Pass self.registered_task_tools instead of agent.registered_agent_tools
            removed_tool_names, removed_objects = agent.tool_manager.remove_tools(
                tools=regular_tools_to_remove,
                registered_tools=self.registered_task_tools
            )
            
            # Update self.registered_task_tools - remove the tool names
            for tool_name in removed_tool_names:
                if tool_name in self.registered_task_tools:
                    del self.registered_task_tools[tool_name]
        
        # Handle builtin tools separately - they don't use ToolManager/ToolProcessor
        if builtin_tools_to_remove and hasattr(self, 'task_builtin_tools'):
            # Remove from task_builtin_tools by unique_id
            builtin_ids_to_remove = {tool.unique_id for tool in builtin_tools_to_remove}
            self.task_builtin_tools = [
                tool for tool in self.task_builtin_tools 
                if tool.unique_id not in builtin_ids_to_remove
            ]
            # Add to removed_objects for self.tools cleanup
            removed_objects.extend(builtin_tools_to_remove)
        
        # Update self.tools - remove all removed objects (regular + builtin)
        if self.tools and removed_objects:
            self.tools = [t for t in self.tools if t not in removed_objects]

    @property
    def context_formatted(self) -> Optional[str]:
        """
        Provides read-only access to the formatted context string.
        
        This property retrieves the value of the internal `_context_formatted`
        attribute, which is expected to be populated by a context management
        process before task execution.
        """
        return self._context_formatted

    @property
    def tools_awaiting_external_execution(self) -> List["ExternalToolCall"]:
        """
        Get the list of tool calls awaiting external execution.
        When the task is paused, this list should be iterated over,
        the tools executed, and the 'result' attribute of each item set.
        """
        return self._tools_awaiting_external_execution
    
    
    @property
    def run_id(self) -> Optional[str]:
        """
        Get the run ID associated with this task.
        
        This is set when the task is executed and allows the task to be
        used for continuation even with a new agent instance.
        
        Returns:
            The run_id if set, None otherwise
        """
        return self._run_id
    
    @run_id.setter
    def run_id(self, value: Optional[str]) -> None:
        """Set the run ID for this task."""
        self._run_id = value
    
    @context_formatted.setter
    def context_formatted(self, value: Optional[str]):
        """
        Sets the internal `_context_formatted` attribute.

        This allows an external process, like a ContextManager, to set the
        final formatted context string on the task object using natural
        attribute assignment syntax.

        Args:
            value: The formatted context string to be assigned.
        """
        self._context_formatted = value
    
    async def additional_description(self, client):
        if not self.context:
            return ""
        
        # Lazy import for heavy modules
        from upsonic.knowledge_base.knowledge_base import KnowledgeBase
            
        rag_results = []
        for context in self.context:
            
            # Lazy import KnowledgeBase to avoid heavy imports
            if hasattr(context, 'rag') and context.rag == True:
                # Import KnowledgeBase only when needed
                if isinstance(context, KnowledgeBase):
                    await context.setup_rag()
                    rag_result_objects = await context.query_async(self.description, task=self)
                    # Convert RAGSearchResult objects to formatted strings
                    if rag_result_objects:
                        formatted_results = []
                        for i, result in enumerate(rag_result_objects, 1):
                            cleaned_text = result.text.strip()
                            metadata_str = ""
                            if result.metadata:
                                source = result.metadata.get('source', 'Unknown')
                                page_number = result.metadata.get('page_number')
                                chunk_id = result.chunk_id or result.metadata.get('chunk_id')
                                
                                metadata_parts = [f"source: {source}"]
                                if page_number is not None:
                                    metadata_parts.append(f"page: {page_number}")
                                if chunk_id:
                                    metadata_parts.append(f"chunk_id: {chunk_id}")
                                if result.score is not None:
                                    metadata_parts.append(f"score: {result.score:.3f}")
                                
                                metadata_str = f" [metadata: {', '.join(metadata_parts)}]"
                            
                            formatted_results.append(f"[{i}]{metadata_str} {cleaned_text}")
                        
                        rag_results.extend(formatted_results)
                
        if rag_results:
            return f"The following is the RAG data: <rag>{' '.join(rag_results)}</rag>"
        return ""


    @property
    def attachments_base64(self):
        """
        Convert all attachment files to base64 encoded strings.
        
        Base64 encoding works with any file type (images, PDFs, documents, etc.)
        and is commonly used for embedding binary data in text-based formats.
        
        Returns:
            List[str]: List of base64 encoded strings, one for each attachment
            None: If no attachments are present
        """
        if self.attachments is None:
            return None
        base64_attachments = []
        for attachment_path in self.attachments:
            try:
                with open(attachment_path, "rb") as attachment_file:
                    file_data = attachment_file.read()
                    base64_encoded = base64.b64encode(file_data).decode('utf-8')
                    base64_attachments.append(base64_encoded)
            except Exception as e:
                # Log the error but continue with other attachments
                from upsonic.utils.printing import warning_log
                warning_log(f"Could not encode attachment {attachment_path} to base64: {e}", "TaskProcessor")
        return base64_attachments


    @property
    def price_id(self):
        if self.price_id_ is None:
            import uuid
            self.price_id_ = str(uuid.uuid4())
        return self.price_id_

    @property
    def task_id(self):
        if self.task_id_ is None:
            import uuid
            self.task_id_ = str(uuid.uuid4())
        return self.task_id_
    
    def get_task_id(self):
        return f"Task_{self.task_id[:8]}"

    @property
    def response(self):

        if self._response is None:
            return None

        if type(self._response) == str:
            return self._response



        return self._response



    def get_total_cost(self):
        if self.price_id_ is None:
            return None
        # Lazy import for heavy modules
        from upsonic.utils.printing import get_price_id_total_cost
        return get_price_id_total_cost(self.price_id)
    
    @property
    def total_cost(self) -> Optional[float]:
        """
        Get the total estimated cost of this task.
        
        Returns:
            Optional[float]: The estimated cost in USD, or None if not available
        """
        the_total_cost = self.get_total_cost()
        if the_total_cost and "estimated_cost" in the_total_cost:
            return the_total_cost["estimated_cost"]
        return None
        
    @property
    def total_input_token(self) -> Optional[int]:
        """
        Get the total number of input tokens used by this task.
        
        Returns:
            Optional[int]: The number of input tokens, or None if not available
        """
        the_total_cost = self.get_total_cost()
        if the_total_cost and "input_tokens" in the_total_cost:
            return the_total_cost["input_tokens"]
        return None
        
    @property
    def total_output_token(self) -> Optional[int]:
        """
        Get the total number of output tokens used by this task.
        
        Returns:
            Optional[int]: The number of output tokens, or None if not available
        """
        the_total_cost = self.get_total_cost()
        if the_total_cost and "output_tokens" in the_total_cost:
            return the_total_cost["output_tokens"]
        return None

    @property
    def cache_hit(self) -> bool:
        """
        Check if the last response was retrieved from cache.
        
        Returns:
            bool: True if the response came from cache, False otherwise
        """
        return self._cache_hit

    @property
    def tool_calls(self) -> List[Dict[str, Any]]:
        """
        Get all tool calls made during this task's execution.
        
        Returns:
            List[Dict[str, Any]]: A list of dictionaries containing information about tool calls,
            including tool name, parameters, and result.
        """
        if self._tool_calls is None:
            self._tool_calls = []
        return self._tool_calls
        
    def add_tool_call(self, tool_call: Dict[str, Any]) -> None:
        """
        Add a tool call to the task's history.
        
        Args:
            tool_call (Dict[str, Any]): Dictionary containing information about the tool call.
                Should include 'tool_name', 'params', and 'tool_result' keys.
        """
        if self._tool_calls is None:
            self._tool_calls = []
        self._tool_calls.append(tool_call)



    def canvas_agent_description(self):
        return "You are a canvas agent. You have tools. You can edit the canvas and get the current text of the canvas."

    def add_canvas(self, canvas):
        # Check if canvas tools have already been added to prevent duplicates
        canvas_functions = canvas.functions()
        canvas_description = self.canvas_agent_description()
        
        # Check if canvas tools are already present
        canvas_already_added = False
        if canvas_functions:
            # Check if any of the canvas functions are already in tools
            for canvas_func in canvas_functions:
                if canvas_func in self.tools:
                    canvas_already_added = True
                    break
        
        # Only add canvas tools if they haven't been added before
        if not canvas_already_added:
            self.tools += canvas_functions
            
        # Check if canvas description is already in the task description
        if canvas_description not in self.description:
            self.description += canvas_description



    def task_start(self, agent):
        self.start_time = time.time()
        if agent.canvas:
            self.add_canvas(agent.canvas)


    def task_end(self):
        self.end_time = time.time()

    def task_response(self, model_response):
        self._response = model_response.output




    
    def set_cache_manager(self, cache_manager: Any):
        """Set the cache manager for this task."""
        self._cache_manager = cache_manager
    
    async def get_cached_response(self, input_text: str, llm_provider: Optional[Any] = None) -> Optional[Any]:
        """
        Get cached response for the given input text.
        
        Args:
            input_text: The input text to search for in cache
            llm_provider: LLM provider for semantic comparison (for llm_call method)
            
        Returns:
            Cached response if found, None otherwise
        """
        if not self.enable_cache or not self._cache_manager:
            return None
        
        cached_response = await self._cache_manager.get_cached_response(
            input_text=input_text,
            cache_method=self.cache_method,
            cache_threshold=self.cache_threshold,
            duration_minutes=self.cache_duration_minutes,
            embedding_provider=self.cache_embedding_provider,
            llm_provider=llm_provider
        )
        
        if cached_response is not None:
            self._cache_hit = True
            self._last_cache_entry = {"output": cached_response}
        
        return cached_response
    
    async def store_cache_entry(self, input_text: str, output: Any):
        """
        Store a new cache entry.
        
        Args:
            input_text: The input text
            output: The corresponding output
        """
        if not self.enable_cache or not self._cache_manager:
            return
        
        await self._cache_manager.store_cache_entry(
            input_text=input_text,
            output=output,
            cache_method=self.cache_method,
            embedding_provider=self.cache_embedding_provider
        )
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        if not self._cache_manager:
            return {
                "total_entries": 0,
                "cache_hits": 0,
                "cache_misses": 0,
                "hit_rate": 0.0,
                "cache_method": self.cache_method,
                "cache_threshold": self.cache_threshold,
                "cache_duration_minutes": self.cache_duration_minutes,
                "session_id": None
            }
        
        stats = self._cache_manager.get_cache_stats()
        stats.update({
            "cache_method": self.cache_method,
            "cache_threshold": self.cache_threshold,
            "cache_duration_minutes": self.cache_duration_minutes,
            "cache_hit": self._cache_hit
        })
        
        return stats
    
    def clear_cache(self):
        """Clear all cache entries."""
        if self._cache_manager:
            self._cache_manager.clear_cache()
        self._cache_hit = False
    
    def _serialize_callable(self, obj: Any) -> Any:
        """Serialize a callable or object to a comparable format using cloudpickle."""
        if obj is None:
            return None
        try:
            import cloudpickle
            import base64
            # Serialize to bytes and encode to base64 string
            pickled = cloudpickle.dumps(obj)
            return {"__callable_pickled__": base64.b64encode(pickled).decode('utf-8')}
        except Exception:
            # If can't pickle, return string representation
            return {"__callable_repr__": repr(obj)}
    
    def _serialize_tools(self, tools: Optional[List[Any]]) -> Optional[List[Any]]:
        """Serialize a list of tools (which may be callables or class instances)."""
        if tools is None:
            return None
        return [self._serialize_callable(t) for t in tools]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary (pure transformation). Callables are serialized for comparison."""
        return {
            "description": self.description,
            "attachments": self.attachments,
            "tools": self._serialize_tools(self.tools),
            "response_format": str(self.response_format) if self.response_format else None,
            "response_lang": self.response_lang,
            "context": self.context,
            "price_id_": self.price_id_,
            "task_id_": self.task_id_,
            "not_main_task": self.not_main_task,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "agent": self._serialize_callable(self.agent) if self.agent else None,
            "enable_thinking_tool": self.enable_thinking_tool,
            "enable_reasoning_tool": self.enable_reasoning_tool,
            "guardrail": self._serialize_callable(self.guardrail),
            "guardrail_retries": self.guardrail_retries,
            "is_paused": self.is_paused,
            "enable_cache": self.enable_cache,
            "cache_method": self.cache_method,
            "cache_threshold": self.cache_threshold,
            "cache_duration_minutes": self.cache_duration_minutes,
            "vector_search_top_k": self.vector_search_top_k,
            "vector_search_alpha": self.vector_search_alpha,
            "vector_search_fusion_method": self.vector_search_fusion_method,
            "vector_search_similarity_threshold": self.vector_search_similarity_threshold,
            "vector_search_filter": self.vector_search_filter,
            "_response": self._response,
            "_context_formatted": self._context_formatted,
            "_tool_calls": self._tool_calls,
            "_tools_awaiting_external_execution": self._tools_awaiting_external_execution,
            "_cache_hit": self._cache_hit,
            "_original_input": self._original_input,
            "_run_id": self._run_id,
            "registered_task_tools": {k: self._serialize_callable(v) for k, v in self.registered_task_tools.items()} if self.registered_task_tools else {},
            "task_builtin_tools": self._serialize_tools(self.task_builtin_tools),
        }
    
    @classmethod
    def _deserialize_callable(cls, obj: Any) -> Any:
        """Deserialize a callable from the serialized format."""
        if obj is None:
            return None
        if isinstance(obj, dict):
            if "__callable_pickled__" in obj:
                try:
                    import cloudpickle
                    import base64
                    pickled_bytes = base64.b64decode(obj["__callable_pickled__"].encode('utf-8'))
                    return cloudpickle.loads(pickled_bytes)
                except Exception:
                    return None
            elif "__callable_repr__" in obj:
                # Can't restore from repr, return None
                return None
        return obj
    
    @classmethod
    def _deserialize_tools(cls, tools: Optional[List[Any]]) -> Optional[List[Any]]:
        """Deserialize a list of tools from the serialized format."""
        if tools is None:
            return None
        return [cls._deserialize_callable(t) for t in tools]
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Task":
        """Reconstruct from dictionary (pure transformation)."""
        # Deserialize callables first
        tools = cls._deserialize_tools(data.get("tools"))
        guardrail = cls._deserialize_callable(data.get("guardrail"))
        agent = cls._deserialize_callable(data.get("agent"))
        
        # Handle response_format (could be a string representation of a class)
        response_format = data.get("response_format")
        if isinstance(response_format, str) and response_format.startswith("<class"):
            # It was serialized as str(class), default to str
            response_format = str
        
        # Filter to only fields that Task accepts in constructor (exclude computed fields)
        valid_fields = {
            "description", "attachments", "response_lang",
            "context", "price_id_", "task_id_", "not_main_task", "start_time", "end_time",
            "enable_thinking_tool", "enable_reasoning_tool",
            "guardrail_retries", "is_paused", "enable_cache", "cache_method",
            "cache_threshold", "cache_duration_minutes", "vector_search_top_k",
            "vector_search_alpha", "vector_search_fusion_method",
            "vector_search_similarity_threshold", "vector_search_filter"
        }
        filtered_data = {k: v for k, v in data.items() if k in valid_fields}
        
        # Convert float timestamps to int (required by Pydantic validation)
        if "start_time" in filtered_data and filtered_data["start_time"] is not None:
            filtered_data["start_time"] = int(filtered_data["start_time"])
        if "end_time" in filtered_data and filtered_data["end_time"] is not None:
            filtered_data["end_time"] = int(filtered_data["end_time"])
        
        # Add deserialized fields
        filtered_data["tools"] = tools
        filtered_data["guardrail"] = guardrail
        filtered_data["agent"] = agent
        filtered_data["response_format"] = response_format
        
        task = cls(**filtered_data)
        
        # Restore computed/private fields
        registered_task_tools = data.get("registered_task_tools")
        if registered_task_tools:
            if isinstance(registered_task_tools, dict):
                # Deserialize each tool in the dict
                task.registered_task_tools = {
                    k: cls._deserialize_callable(v) for k, v in registered_task_tools.items()
                }
            else:
                task.registered_task_tools = registered_task_tools
        
        task_builtin_tools = data.get("task_builtin_tools")
        if task_builtin_tools:
            task.task_builtin_tools = cls._deserialize_tools(task_builtin_tools)
        
        if data.get("_response") is not None:
            task._response = data["_response"]
        if data.get("_context_formatted") is not None:
            task._context_formatted = data["_context_formatted"]
        if data.get("_tool_calls") is not None:
            task._tool_calls = data["_tool_calls"]
        if data.get("_tools_awaiting_external_execution") is not None:
            task._tools_awaiting_external_execution = data["_tools_awaiting_external_execution"]
        if data.get("_cache_hit") is not None:
            task._cache_hit = data["_cache_hit"]
        if data.get("_original_input") is not None:
            task._original_input = data["_original_input"]
        if data.get("_run_id") is not None:
            task._run_id = data["_run_id"]
        
        return task
    
    def serialize(self) -> bytes:
        """Serialize to bytes for storage."""
        import cloudpickle
        return cloudpickle.dumps(self.to_dict())
    
    @classmethod
    def deserialize(cls, data: bytes) -> "Task":
        """Deserialize from bytes."""
        import cloudpickle
        dict_data = cloudpickle.loads(data)
        return cls.from_dict(dict_data)


def _rebuild_task_model():
    """Rebuild Task model after all dependencies are imported."""
    try:
        Task.model_rebuild()
    except Exception:
        pass
_rebuild_task_model()